{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-03T23:16:46.806763Z",
     "start_time": "2025-12-03T23:16:44.560033Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T00:03:10.586915Z",
     "start_time": "2025-12-04T00:03:10.555495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "old_df = pd.read_csv(\"C:/Users/ki87ujmn/Downloads/merged_file.csv\")\n",
    "mapping = old_df.set_index('site_id')['weir_length']\n"
   ],
   "id": "ce3c880dd0fb9615",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T00:03:26.344534Z",
     "start_time": "2025-12-04T00:03:25.956683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_incidents = pd.read_excel(io='../data/Master Copy of LHD Fatalities Database.xlsx',\n",
    "                       sheet_name='Incidents')\n",
    "# let's see how many fatalities we have in the database...\n",
    "print(f'There are {len(raw_incidents)} fatalities in the fatalities database.')"
   ],
   "id": "ba262dcb474c05e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 600 fatalities in the fatalities database.\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T00:03:49.602706Z",
     "start_time": "2025-12-04T00:03:49.592860Z"
    }
   },
   "cell_type": "code",
   "source": "raw_incidents['weir_length'] = raw_incidents['site_id'].map(mapping)",
   "id": "b269067bf68f9621",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T00:03:51.762895Z",
     "start_time": "2025-12-04T00:03:51.736563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now let's see how many have complete dates\n",
    "date_cols = [\"day\", \"month\", \"year\"]\n",
    "incident_df = raw_incidents[(raw_incidents[date_cols] != 0).all(axis=1)].copy()\n",
    "incident_df[\"date\"] = pd.to_datetime(incident_df[[\"year\", \"month\", \"day\"]])\n",
    "incident_df[\"date\"] = incident_df[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "print(f'There are {len(incident_df)} fatalities with complete dates.')"
   ],
   "id": "6df31bd8bcdea798",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 550 fatalities with complete dates.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T00:03:53.592268Z",
     "start_time": "2025-12-04T00:03:53.321215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let's look at the locations now...\n",
    "site_df = pd.read_excel(io='../data/Master Copy of LHD Fatalities Database.xlsx',\n",
    "                       sheet_name='Sites')\n",
    "# let's see how many dams we have in the database...\n",
    "print(f'There are {len(site_df)} fatalities in the fatalities database.')"
   ],
   "id": "639c198fce52d5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 356 fatalities in the fatalities database.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:06:05.542567Z",
     "start_time": "2025-12-04T22:06:05.450026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# long_df = site_df.merge(incident_df[[\"site_id\", \"date\"]], on=\"site_id\", how=\"left\")\n",
    "# long_df = long_df.dropna(subset=[\"date\"])\n",
    "long_df = pd.read_excel('../data/LHD Sites and Fatalities.xlsx')\n",
    "\n",
    "print(f\"There are {len(long_df)} fatalities with complete dates.\\n\"\n",
    "      f\"There are {long_df['site_id'].nunique()} sites with complete dates.\")\n",
    "long_df = long_df[long_df[\"comments\"] != \"removed\"]\n",
    "print(f\"There are {len(long_df)} fatalities at current low-head dams.\\n\"\n",
    "      f\"There are {long_df['site_id'].nunique()} current low-head dams.\")\n",
    "# long_df.to_excel(excel_writer='../data/LHD Sites and Fatalities.xlsx',\n",
    "#                  sheet_name='Fatalities', index=False)"
   ],
   "id": "dba2c97328074a77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 550 fatalities with complete dates.\n",
      "There are 344 sites with complete dates.\n",
      "There are 449 fatalities at current low-head dams.\n",
      "There are 284 current low-head dams.\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:06:13.103531Z",
     "start_time": "2025-12-04T22:06:12.558576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now let's see how many we can test with the NWM\n",
    "start = \"1979-02-01\"\n",
    "end   = \"2023-01-31\"\n",
    "nwm_df = long_df[(long_df[\"date\"] >= start) & (long_df[\"date\"] <= end)]\n",
    "print(f\"There are {len(nwm_df)} fatalities with NWM-compatible dates.\\n\"\n",
    "      f\"There are {nwm_df['site_id'].nunique()} sites with NWM-compatible dates.\")\n",
    "nwm_df.to_excel(excel_writer='../data/NWM LHD Sites and Fatalities.xlsx',\n",
    "                 sheet_name='Fatalities', index=False)"
   ],
   "id": "9bf3a575aba2dfc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 372 fatalities with NWM-compatible dates.\n",
      "There are 264 sites with NWM-compatible dates.\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T22:06:17.571387Z",
     "start_time": "2025-12-04T22:06:17.468249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now let's see how many we can test with the NWM\n",
    "geo_df = long_df[long_df[\"date\"] >= \"1940-01-01\"]\n",
    "print(f\"There are {len(geo_df)} fatalities with GEOGLOWS-compatible dates.\\n\"\n",
    "      f\"There are {geo_df['site_id'].nunique()} sites with GEOGLOWS-compatible dates.\")\n",
    "geo_df.to_excel(excel_writer='../data/GEOGLOWS LHD Sites and Fatalities.xlsx',\n",
    "                 sheet_name='Fatalities', index=False)"
   ],
   "id": "7ad1e6abe767f8de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 434 fatalities with GEOGLOWS-compatible dates.\n",
      "There are 281 sites with GEOGLOWS-compatible dates.\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T20:09:41.357200Z",
     "start_time": "2025-12-08T20:09:41.154068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the excel file\n",
    "file_path = '/Users/kennyquintana/Developer/lhd-processor/lhd_processor/data/LHD Sites and Fatalities copy.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 2. Define the columns for each group\n",
    "site_columns = [\n",
    "    'site_id', 'name', 'latitude', 'longitude',\n",
    "    'city', 'county', 'state', 'weir_length', 'comments'\n",
    "]\n",
    "\n",
    "incident_columns = [\n",
    "    'site_id', 'date'\n",
    "]\n",
    "\n",
    "# 3. Create the Site-Specific DataFrame\n",
    "# We drop duplicates on 'site_id' so each dam is listed only once\n",
    "sites_df = df[site_columns].drop_duplicates(subset='site_id').reset_index(drop=True)\n",
    "\n",
    "# 4. Create the Incident-Specific DataFrame\n",
    "# We keep all rows here, as multiple incidents can occur at one site_id\n",
    "incidents_df = df[incident_columns].reset_index(drop=True)\n",
    "\n",
    "# 5. Export to an Excel file with two sheets\n",
    "output_filename = '/Users/kennyquintana/Developer/lhd-processor/lhd_processor/data/LHD_Split_Data.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "    sites_df.to_excel(writer, sheet_name='Sites', index=False)\n",
    "    incidents_df.to_excel(writer, sheet_name='Incidents', index=False)\n",
    "\n",
    "print(f\"Successfully created {output_filename} with two sheets.\")\n",
    "print(f\"Unique Sites: {len(sites_df)}\")\n",
    "print(f\"Total Incidents: {len(incidents_df)}\")"
   ],
   "id": "d5811cab1121d433",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created /Users/kennyquintana/Developer/lhd-processor/lhd_processor/data/LHD_Split_Data.xlsx with two sheets.\n",
      "Unique Sites: 344\n",
      "Total Incidents: 550\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:55:54.738932Z",
     "start_time": "2025-12-08T22:55:53.391399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Configuration: Set your Date Ranges Here ---\n",
    "# Adjust these based on the specific Reanalysis version you are using.\n",
    "# NWM Retrospective 2.1 is typically 1979-02-01 to 2020-12-31.\n",
    "# NWM 3.0 extends further.\n",
    "NWM_START_DATE = '1979-02-01'\n",
    "NWM_END_DATE = '2023-01-31'\n",
    "\n",
    "# GEOGLOWS v2 typically goes back to 1940.\n",
    "GEOGLOWS_START_DATE = '1940-01-01'\n",
    "\n",
    "def process_and_split_data(input_path):\n",
    "    print(f\"Reading: {input_path}\")\n",
    "    xls = pd.ExcelFile(input_path)\n",
    "\n",
    "    # 1. Load Data\n",
    "    df_sites = pd.read_excel(xls, 'Sites')\n",
    "    df_incidents = pd.read_excel(xls, 'Incidents')\n",
    "\n",
    "    # 2. Universal Cleaning: Remove rows where comments == 'removed'\n",
    "    # Check 'sites'\n",
    "    if 'comments' in df_sites.columns:\n",
    "        df_sites = df_sites[df_sites['comments'].astype(str).str.lower().str.strip() != 'removed']\n",
    "\n",
    "    # Check 'incidents'\n",
    "    if 'comments' in df_incidents.columns:\n",
    "        df_incidents = df_incidents[df_incidents['comments'].astype(str).str.lower().str.strip() != 'removed']\n",
    "\n",
    "    # 3. Date Filtering Preparation\n",
    "    # Ensure the date column is actually datetime objects.\n",
    "    date_col = 'date'\n",
    "    df_incidents[date_col] = pd.to_datetime(df_incidents[date_col], errors='coerce')\n",
    "\n",
    "    # Drop rows where date failed to parse (NaT) if necessary\n",
    "    df_incidents = df_incidents.dropna(subset=[date_col])\n",
    "\n",
    "    # 4. Create NWM Slice\n",
    "    mask_nwm = (df_incidents[date_col] >= NWM_START_DATE) & (df_incidents[date_col] <= NWM_END_DATE)\n",
    "    df_incidents_nwm = df_incidents.loc[mask_nwm]\n",
    "\n",
    "    # 5. Create GEOGLOWS Slice\n",
    "    mask_geoglows = df_incidents[date_col] >= GEOGLOWS_START_DATE\n",
    "    df_incidents_geoglows = df_incidents.loc[mask_geoglows]\n",
    "\n",
    "    print(f\"Total Incidents (Cleaned): {len(df_incidents)}\")\n",
    "    print(f\"NWM Compatible Incidents: {len(df_incidents_nwm)}\")\n",
    "    print(f\"GEOGLOWS Compatible Incidents: {len(df_incidents_geoglows)}\")\n",
    "\n",
    "    # 6. Save NWM File (Sites + Filtered Incidents)\n",
    "    with pd.ExcelWriter('/Volumes/KenDrive/LHD_Project/nwm_data.xlsx', engine='openpyxl') as writer:\n",
    "        df_sites.to_excel(writer, sheet_name='Sites', index=False)\n",
    "        df_incidents_nwm.to_excel(writer, sheet_name='Incidents', index=False)\n",
    "\n",
    "    # 7. Save GEOGLOWS File (Sites + Filtered Incidents)\n",
    "    with pd.ExcelWriter('/Volumes/KenDrive/LHD_Project/geoglows_data.xlsx', engine='openpyxl') as writer:\n",
    "        df_sites.to_excel(writer, sheet_name='Sites', index=False)\n",
    "        df_incidents_geoglows.to_excel(writer, sheet_name='Incidents', index=False)\n",
    "\n",
    "    print(\"\\nSuccess! Created 'nwm_data.xlsx' and 'geoglows_data.xlsx'.\")\n",
    "\n",
    "# Run the function\n",
    "# Replace 'master_data.xlsx' with your actual file name\n",
    "process_and_split_data('/Volumes/KenDrive/LHD_Project/LHD_Database_Full.xlsx')"
   ],
   "id": "c1b249636154c045",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /Volumes/KenDrive/LHD_Project/LHD_Database_Full.xlsx\n",
      "Total Incidents (Cleaned): 550\n",
      "NWM Compatible Incidents: 460\n",
      "GEOGLOWS Compatible Incidents: 534\n",
      "\n",
      "Success! Created 'nwm_data.xlsx' and 'geoglows_data.xlsx'.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T00:55:44.929916Z",
     "start_time": "2025-12-09T00:55:44.660155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Read the entire Excel file. sheet_name=None reads all sheets into a dictionary.\n",
    "input_file = '/Volumes/KenDrive/LHD_Project/nwm_data.xlsx'\n",
    "all_sheets = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "# 2. Extract the relevant DataFrames\n",
    "sites_df = all_sheets['Sites']\n",
    "incidents_df = all_sheets['Incidents']\n",
    "\n",
    "# 3. Get the valid site_ids from the 'Sites' tab\n",
    "valid_site_ids = sites_df['site_id'].unique()\n",
    "\n",
    "# 4. Filter the 'Incidents' tab\n",
    "# Keep only rows where 'site_id' is in the list of valid_site_ids\n",
    "filtered_incidents_df = incidents_df[incidents_df['site_id'].isin(valid_site_ids)]\n",
    "\n",
    "# 5. Update the dictionary with the filtered dataframe\n",
    "all_sheets['Incidents'] = filtered_incidents_df\n",
    "\n",
    "# 6. Write all sheets (including the unchanged ones) to a new Excel file\n",
    "output_file = '/Volumes/KenDrive/LHD_Project/nwm_data.xlsx'\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for sheet_name, df in all_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Successfully saved filtered data to {output_file}\")"
   ],
   "id": "b7222ec05f47cbb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved filtered data to /Volumes/KenDrive/LHD_Project/nwm_data.xlsx\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T01:50:05.058713Z",
     "start_time": "2025-12-09T01:50:04.493688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Read the entire Excel file.\n",
    "input_file = '/Volumes/KenDrive/LHD_Project/geoglows_data.xlsx'\n",
    "all_sheets = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "# 2. Extract the relevant DataFrames\n",
    "sites_df = all_sheets['Sites']\n",
    "incidents_df = all_sheets['Incidents']\n",
    "\n",
    "# 3. Get the valid site_ids from the 'Incidents' tab (The Source)\n",
    "# This identifies which sites actually have recorded incidents\n",
    "incident_site_ids = incidents_df['site_id'].unique()\n",
    "\n",
    "# 4. Filter the 'Sites' tab (The Target)\n",
    "# Keep only rows in 'Sites' where the 'site_id' appears in the Incidents list\n",
    "filtered_sites_df = sites_df[sites_df['site_id'].isin(incident_site_ids)]\n",
    "\n",
    "# 5. Update the dictionary with the filtered dataframe\n",
    "all_sheets['Sites'] = filtered_sites_df\n",
    "\n",
    "# 6. Write all sheets to the Excel file\n",
    "output_file = '/Volumes/KenDrive/LHD_Project/geoglows_data_test.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for sheet_name, df in all_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Successfully filtered 'Sites' based on 'Incidents' and saved to {output_file}\")\n",
    "print(f\"Original Sites count: {len(sites_df)}\")\n",
    "print(f\"Filtered Sites count: {len(filtered_sites_df)}\")"
   ],
   "id": "15acf14b07f824ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully filtered 'Sites' based on 'Incidents' and saved to /Volumes/KenDrive/LHD_Project/geoglows_data_test.xlsx\n",
      "Original Sites count: 284\n",
      "Filtered Sites count: 281\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
