{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-31T04:49:39.224118Z",
     "start_time": "2025-12-31T04:49:02.348805Z"
    }
   },
   "source": [
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pyproj\n",
    "\n",
    "# --- NEW: Fix PROJ Environment for Micromamba/Conda ---\n",
    "try:\n",
    "    # Try to find the database path automatically based on your Micromamba environment\n",
    "    prefix = os.environ.get('CONDA_PREFIX')\n",
    "    if not prefix:\n",
    "        # Fallback to the path seen in your error message\n",
    "        prefix = '/opt/homebrew/Cellar/micromamba/2.4.0/envs/lhd-environment'\n",
    "\n",
    "    proj_path = os.path.join(prefix, 'share', 'proj')\n",
    "    if os.path.exists(proj_path):\n",
    "        os.environ['PROJ_LIB'] = proj_path\n",
    "        pyproj.datadir.set_data_dir(proj_path)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 1. Load the Excel Master List\n",
    "excel_path = '/Users/kennyquintana/Downloads/merged_sites_clean.xlsx'\n",
    "df_sites = pd.read_excel(excel_path, sheet_name='Sites')\n",
    "df_sites.columns = [c.lower() for c in df_sites.columns]\n",
    "\n",
    "nhd_to_tdx_list = []\n",
    "tdx_to_nhd_list = []\n",
    "\n",
    "print(f\"Starting spatial mapping for {len(df_sites)} sites...\")\n",
    "\n",
    "for index, row in tqdm(df_sites.iterrows(), total=len(df_sites)):\n",
    "    try:\n",
    "        nhd_path = row.get('flowline_path_nhd')\n",
    "        tdx_path = row.get('flowline_path_tdx')\n",
    "\n",
    "        if pd.isna(nhd_path) or pd.isna(tdx_path):\n",
    "            continue\n",
    "        if not os.path.exists(str(nhd_path)) or not os.path.exists(str(tdx_path)):\n",
    "            continue\n",
    "\n",
    "        nhd = gpd.read_file(nhd_path)\n",
    "        tdx = gpd.read_file(tdx_path)\n",
    "\n",
    "        # Auto-lowercase headers\n",
    "        nhd.columns = [c.lower() for c in nhd.columns]\n",
    "        tdx.columns = [c.lower() for c in tdx.columns]\n",
    "\n",
    "        # RE-INTRODUCED PROJECTION:\n",
    "        # Using a meter-based projection for accurate 'nearest' calculations.\n",
    "        # If 5070 still fails, it will fall back to the degrees (with the warning)\n",
    "        try:\n",
    "            nhd = nhd.to_crs(epsg=5070)\n",
    "            tdx = tdx.to_crs(epsg=5070)\n",
    "        except Exception:\n",
    "            # Fallback: just ensure they match each other if the DB error persists\n",
    "            if nhd.crs != tdx.crs:\n",
    "                tdx = tdx.to_crs(nhd.crs)\n",
    "\n",
    "        # 4. Spatial Join (Nearest TDX line for every NHD segment)\n",
    "        mapped = gpd.sjoin_nearest(nhd, tdx, distance_col=\"dist\")\n",
    "\n",
    "        # Determine TDX ID column (usually 'linkno', 'river_id', or 'comid')\n",
    "        possible_ids = ['linkno', 'river_id', 'comid']\n",
    "        tdx_id_col = next((c for c in possible_ids if c in mapped.columns), mapped.columns[-1])\n",
    "\n",
    "        # --- MAP 1: NHD -> GEOGLOWS (Many-to-One) ---\n",
    "        map1 = mapped[['nhdplusid', tdx_id_col]].copy()\n",
    "        nhd_to_tdx_list.append(map1)\n",
    "\n",
    "        # --- MAP 2: GEOGLOWS -> NHD (One-to-One Outlet) ---\n",
    "        # Use hydroseq if available, otherwise just pick one\n",
    "        sort_col = 'hydroseq' if 'hydroseq' in mapped.columns else mapped.columns[0]\n",
    "\n",
    "        map2 = mapped.sort_values(sort_col, ascending=True) \\\n",
    "                     .drop_duplicates(subset=[tdx_id_col])\n",
    "\n",
    "        map2 = map2[[tdx_id_col, 'nhdplusid']].copy()\n",
    "        tdx_to_nhd_list.append(map2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError at index {index} (Site ID: {row.get('site_id')}): {e}\")\n",
    "\n",
    "# 5. Concatenate and Save\n",
    "if nhd_to_tdx_list:\n",
    "    final_nhd_to_tdx = pd.concat(nhd_to_tdx_list).drop_duplicates()\n",
    "    final_tdx_to_nhd = pd.concat(tdx_to_nhd_list).drop_duplicates()\n",
    "\n",
    "    final_nhd_to_tdx.to_csv('nhd_to_geoglows_crosswalk.csv', index=False)\n",
    "    final_tdx_to_nhd.to_csv('geoglows_to_nhd_crosswalk.csv', index=False)\n",
    "    print(f\"\\nSuccess! Mapped {len(final_nhd_to_tdx)} unique segments.\")\n",
    "else:\n",
    "    print(\"\\nNo data was successfully processed.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting spatial mapping for 256 sites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:36<00:00,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success! Mapped 761 unique segments.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
